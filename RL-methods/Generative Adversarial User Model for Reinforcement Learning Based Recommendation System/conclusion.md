motivation:从某种意义上讲，推荐行为其实是通过凸显特定物品并隐藏其他物品来引导用户兴趣的。因此，设计推荐策略会更好一点，比如基于强化学习（RL）的推荐策略——它可以考虑用户的长期兴趣。但由于环境是与已经登陆的在线用户相对应的，因此 RL 框架在推荐系统设置中也遇到了一些挑战。  
challenges:1.驱动用户行为的兴趣点（奖励函数）一般是未知的，但它对于 RL 算法的使用来说至关重要。在用于推荐系统的现有 RL 算法中，奖励函数一般是手动设计的（例如用 ±1 表示点击或不点击），这可能无法反映出用户对不同项目的偏好如何。  
2.model-free的RL一般都需要和环境（在线用户）进行大量的交互才能学到良好的策略。但这在推荐系统设置中是不切实际的。如果推荐看起来比较随机或者推荐结果不符合在线用户兴趣，她会很快放弃这一服务。  
因此，为了解决无模型方法样本复杂度大的问题，基于模型的 RL 方法更为可取。近期有一些研究在相关但不相同的环境设置中训练机器人策略，结果表明基于模型的 RL 采样效率更高。  
contribution:1.开发了生成对抗学习（GAN）方法来模拟用户行为动态并学习其奖励函数。可以通过联合极小化极大优化算法同时评估这两个组件。该方法的优势在于：（i）可以得到更准确的用户模型，而且可以用与用户模型一致的方法学习奖励函数；（ii）相较于手动设计的简单奖励函数，从用户行为中学习到的奖励函数更有利于后面的强化学习；（iii）学习到的用户模型使研究者能够为新用户执行基于模型的 RL 和在线适应从而实现更好的结果。  
2.用这一模型作为模拟环境，研究者还开发了级联 DQN 算法来获得组合推荐策略。动作-价值函数的级联设计允许其在大量候选物品中找到要显示的物品的最佳子集，其时间复杂度和候选物品的数量呈线性关系，大大减少了计算难度。  

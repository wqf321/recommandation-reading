motivation:新闻推荐系统中，新闻具有很强的动态特征（dynamic nature of news features），目前一些模型已经考虑到了动态特征。但是存在如下问题  
1.大多数模型只用点击率CTR来作为目标函数。  
2.鲜少有人尝试利用用户反馈信息来提升推荐效果。  
3.大多数方法都会重复的给用户推荐相似的内容,这会让用户感到无聊。  
因此提出了一个基于深度Q学习的推荐框架，该框架可以明确地模拟未来的奖励，另外，一种有效的探索策略被纳入来为用户寻找新的吸引人的新闻
challenges:1.难以处理新闻推荐的动态变化。这种动态变化体现在两个方面，首先新闻具有很强的时效性，其次是用户对于新闻阅读的兴趣是不断变化的,如下图所示：
图一  因此，在建模过程中，不仅要考虑用户对当前推荐的反馈，还要考虑长期的影响。就好比买股票，不能只考虑眼前的收益，而是要考虑未来的预期收益。
2.当前的推荐算法通常只考虑用户的点击／未点击，或者用户的评分作为反馈，然而，用户隔多久会再次使用服务也能在一定程度上反映用户对推荐结果的满意度。
3.目前的推荐系统倾向于推荐用户重复或相似内容的东西，这也许会降低用户在同一个主题上的兴趣度。因此需要进行exploration。传统方法 e -greedy strategy 或者 Upper Confidence Bound (UCB) 都会在短期对推荐系统的效果造成一定的影响，需要更有效的exploration策略。  
因此，本文提出了基于强化学习的推荐系统框架来解决上述提到的三个问题：  
1.首先，使用DQN网络来有效建模新闻推荐的动态变化属性，DQN可以将短期回报和长期回报进行有效的模拟。  
2.将用户活跃度（activeness score）作为一种新的反馈信息，用户活跃度在后面会详细介绍。  
3.使用Dueling Bandit Gradient Descent方法来进行有效的探索。  
算法框架如下图：  
图二
contribution:  
1.提出了一种强化学习的框架用于在线新闻的个性化推荐。  
2.使用用户活跃度作为一种新的反馈，来提高推荐的准确性。  
3.使用了一种更加高效的探索算法：Dueling Bandit Gra- dient Descent。  
4.模型可以进行在线学习和更新，在离线和在线实验上的表现都超过了传统的算法。  
method:
参考：https://blog.csdn.net/weixin_41864878/article/details/90080576  
https://www.jianshu.com/p/c0384b213320  
https://blog.csdn.net/u9Oo9xkM169LeLDR84/article/details/89325097
